\chapter{Experiments}
\label{sec:experiments}
In this chapter, the definition of various evaluation methods and scenarios performed at the end of training a model is presented. The choice of evaluation methods depends on the initial dataset used for training, as outlined in Chapter \ref{sec:datasets}.

\section{LSM experiments}
The LSM dataset is characterized by a wealth of unique households and a large volume of data samples, providing an opportunity for a comprehensive experimental setup for generative models.
5400 unique households were taken and split into batches of 100 households. We then train the model on the current batch, synthesize samples and then evaluate the quality with different statistical methods.
For instance, lsm 01 batch comprises households from 1 to 100 in the dataset, lsm 02 includes households from 101 to 200, and so forth. The resolution of the time was half hour measurements resulting in samples with 48 timestamps which corresponds to one day. Additionally, two different models were trained to examine the difference between uni- and multivariate models. Consequently, a total of 55 trained models were compared for univariate and 55 for multivariate approaches. The evaluation metrics included the MMD score, as described in section \ref{sec:mmd}, the t-SNE plot outlined in section \ref{sec:tsne}, and various visualizations of daily means, which were then compared to the real data.

\section{OpenMeter experiments}
In contrast to the LSM dataset, the OpenMeter dataset is not curated, and the overall quality of the samples is inconsistent. It exhibits issues such as missing values for different sensors, incoherent feature representation, among others. To facilitate better experimentation and comparability, we opted to define a smaller subset.
The subset includes only sensors categorized as 'private' (the other category being commercial/public), and is also limited to values recorded in the years 2021 and 2022. The sensors in the OpenMeter dataset have a resolution of 15-minute measurements, resulting in 96 values for one day. Univariate and multivariate models are compared, the MMD scores computed, t-SNE plots discussed and daily means for both real and synthetic samples analyzed.

\section{ACN experiments}
For the ACN dataset, a deliberate choice was made to adopt the multivariate approach, ensuring that metrics yield multiple results for each respective dimension. Consequently, daily means are examined for every dimension, the MMD score is computed for each dimension compared to the real data, and one t-SNE plot is discussed. In this case, the dimensional reduction of the t-SNE method can handle multivariate time series samples, allowing for analysis in a single plot.\newline
An applied test was also conducted, examining both real data and synthetic data in a continual learning scenario. Initially, the real data was subjected to testing, involving the training of an arbitrary forecasting model on three different sections corresponding to evenly spread 4-month parts of the initial 2019 jpl dataset. The model was trained using MSE as the loss and EWC, SI, and LWF as three regularization terms which are specific to continuous learning, more specifically regularization-based approaches. After each section, the forecasting performance was evaluated. One assumption was that the model might forget the pre-task of forecasting earlier sections or retain it throughout the experiments. The real data exhibited a certain performance for this continual learning task.Subsequently, the same experiment was replicated with a synthetic JPL dataset. This synthetic data also demonstrated a certain performance or behavior for the continual learning task. Ultimately, a comparison was made, suggesting that the synthetic data behaves similarly to the real data, serving as another indication of good synthetic sample quality. Important to notice is that the following results do not answer the question whether the continual learning performance can be enhanced by involving synthetic data. This is a task for future work which is discussed in section \ref{sec:apply cl}. The applied experiments and their results presented were conducted by Yuan Wang, another student at the IES group. This thesis just discusses the results.