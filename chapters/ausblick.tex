\chapter{Conclusion}
\label{sec:conclusion}
With the proposed methods described in chapter \ref{sec:approach}, authentic samples that resemble the dataset described in chapter \ref{sec:datasets} were successfully generated.
This task was achieved by a DDPM model that is capable of generating time series data. 
By restructuring the UNET to 1-dimensional convolution model, the overall DDPM was able to learn the denoising process described in \ref{sec:ddpm}. Generating time series samples is now possible.
The quality of those generated samples is undermined by the decent scoring in chapter \ref{sec:results}. 
The different described metrics and statistical evaluation methods suggest a good synthetic representation of the initial dataset. 
So for the question of whether DDPM are capable for time series synthesis it is conclude that the assumptions were met positively, but still can be improved through extensive hyper parameter optimization or switching to an other model architecture besides UNET.
Going into the next phase of applicative testing, described in chapter \ref{sec:experiments}, the top performing models where plugged into the described continual learning scenario.
Supplying the forecasting model with synthetic samples to test the training process through different stages of the experiences defined in the 
scenario resulted in an overall similar training performance. The accuracy of the forecasting model was slightly worse during the varying tasks. So our synthetic samples also performed almost identical to real data which is a hint for good quality in a practical setting. Not only is it another indicator for a successfully time series DDPM but also 
can verify the assumption that DDPMs can perform continual learning environment. The overall conclusion is that all assumptions were satisfied.
\section*{Outlook}
Future research topics that are interesting could be more appliance test on other tasks that benefit from synthetic time series samples. 
Also the question of anonymization of sensible time series data in the public or health care sector are fascinating venues that can be explored. And the results in this thesis can be put onto testing in other fields that differ from EV or household power consumption. Since the topic of time series data is a broad plane of potentials the presented approaches and models could be tested for different scenarios in various fields of research. Since the first release of image generation models further improvements were made to them. It is possible to perform different style transfers, upscale images, inpaint or change parts in an image or transform it into another genre keeping the underlying information. Those augmentations should also be possible for the domain of time series data and might present a vault of interesting research questions.\newline
The idea of implementing generative models into continual learning setup was already roughly sketched in section \ref{sec:apply cl}. Using generative models as support models in different CL scenarios may boost the overall CL performance. Retaining previous tasks by supplying compact models instead of entire datasets might be the solution for efficient and sustainable model training in the future. In this case we see generative models as a tool to enhance CL tasks. But it was also imminent that generative models could be seen as the target for CL research. The models are currently trained on temporal data. The overall behavior might change for the coming years, which means that the generative models are getting worse and worse representatives for the real world. So continually improving the models and still retaining the old information is also for generative models an important problem that needs to be investigated.