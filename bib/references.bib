@ARTICLE{TK02,
 author = {Tong, S. and Koller, D.},
 title = {Support Vector Machine Active Learning with Applications to Text Classification},
 journal = {Journal of Machine Learning Research},
 volume = {2},
 year = {2002},
 issn = {1533-7928},
 pages = {45--66},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA}
 }
 
@INPROCEEDINGS{Bri03,
    author = {Brinker, K.},
    title = {Incorporating Diversity in Active Learning with Support Vector Machines},
    booktitle = {IMCL '03: Proceedings of the 20th International Conference on Machine Learning},
    year = {2003},
    pages = {59--66},
    publisher = {AAAI Press},
    address = {Washington, DC, USA}
} 
 
 @MastersThesis{Pau06,
    author = {Pausch, W.},
    title = {{A}ktive und {I}nkrementelle {L}ernverfahren f\"ur {S}upport {V}ector {M}achines und ihre {A}nwendung auf gro\ss{}e {K}lassifikationsaufgaben im {B}ereich des {D}ata {M}ining},
    school = {University of Passau},
    address = {Germany},
    year = {2006},
    month = {August},
}
 
@PHDTHESIS{Jeb02,
	author = {Jebara, T.},
	note = {Supervisor-Pentland, Alex P.},
	keywords = {discriminative, generative, learning, machine},
	posted-at = {2006-12-11 19:44:25},
	school = {MIT},
	title = {Discriminative, Generative and Imitative Learning},
	publisher = {Massachusetts Institute of Technology},
	year = {2002}
}

@BOOK{Bis06,
 author = {Bishop, C. M.},
 title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
 year = {2006},
 isbn = {0387310738},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA}
 }

@TECHREPORT{Ber03,
	author = {Berg, T.  and Jonsson, B.  and Leucker, M.  and Saksena, M. },
	institution = {Department of Information Technology, Uppsala University},
	month = {August},
	number = {2003-039},
	posted-at = {2005-09-05 21:32:21},
	priority = {2},
	title = {Insights to {A}ngluin's Learning},
	year = {2003}
}

 @INCOLLECTION{Pla08,
 title = {A General Agnostic Active Learning Algorithm},
 author = {Dasgupta, S. and Hsu, D. and Monteleoni, C.},
 booktitle = {NIPS '08: Advances in Neural Information Processing Systems 20},
 publisher = {MIT Press},
 address = {Cambridge, MA},
 pages = {353--360},
 year = {2008}
}

@BOOK{Dud00,
	author = {Duda, R. O. and Hart, P. E. and Stork, D. G.},
	howpublished = {Hardcover},
	isbn = {0471056693},
	month = {November},
	publisher = {Wiley-Interscience},
	title = {Pattern Classification (2nd Edition)},
	year = {2000},
}

@UNPUBLISHED{Sic03,
	author = {Sick, B. and Gruber, C.},
	title  ={Soft-Computing (lecture notes)},
	note = {University Passau},
	year = {2003},
}

@MISC{Wel06,
	abstract = {This is a Note to explain Fisher Linear Discriminant Analysis.},
	author = {Welling, M.},
	institution = {University of Toronto},
	organization = {Department of Computer Science},
	title = {Fisher Linear Discriminant Analysis},
	year = {2006},
}
 
 @MISC{Rip96,
	author = {Ripley, B. D.},
	booktitle = {Pattern Recognition and Neural Networks},
 	publisher =  {Cambirgde University Press},
	howpublished = "\url{http://www.stats.ox.ac.uk/pub/PRNN}",
	note = "[Online; accessed 13-October-2009]",
	year = {1996},
}

 @MISC{Ele07,
	author = {UCL},
	title = {{UCL/MLG} {E}lena Database},
 	publisher =  {UCL machine learning group},
	howpublished = "\url{http://www.dice.ucl.ac.be/mlg/?page=elena}",
	note = "[Online; accessed 13-October-2009]",
	year = {2007},
}


@misc{sohl-dickstein_deep_2015,
	title = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
	url = {http://arxiv.org/abs/1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	number = {{arXiv}:1503.03585},
	publisher = {{arXiv}},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	urldate = {2022-07-13},
	date = {2015-11-18},
	eprinttype = {arxiv},
	eprint = {1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Nikita\\Zotero\\storage\\3NATJWGI\\Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nikita\\Zotero\\storage\\UVN8DYGD\\1503.html:text/html},
}

@misc{ho_denoising_2020,
	title = {Denoising Diffusion Probabilistic Models},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional {CIFAR}10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art {FID} score of 3.17. On 256x256 {LSUN}, we obtain sample quality similar to {ProgressiveGAN}. Our implementation is available at https://github.com/hojonathanho/diffusion},
	number = {{arXiv}:2006.11239},
	publisher = {{arXiv}},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	urldate = {2022-07-13},
	date = {2020-12-16},
	eprinttype = {arxiv},
	eprint = {2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Nikita\\Zotero\\storage\\PSRWVEKS\\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nikita\\Zotero\\storage\\URZX7752\\2006.html:text/html},
}

@misc{nichol_improved_2021,
	title = {Improved Denoising Diffusion Probabilistic Models},
	url = {http://arxiv.org/abs/2102.09672},
	abstract = {Denoising diffusion probabilistic models ({DDPM}) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, {DDPMs} can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well {DDPMs} and {GANs} cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
	number = {{arXiv}:2102.09672},
	publisher = {{arXiv}},
	author = {Nichol, Alex and Dhariwal, Prafulla},
	urldate = {2022-07-13},
	date = {2021-02-18},
	eprinttype = {arxiv},
	eprint = {2102.09672 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Nikita\\Zotero\\storage\\FVFK52XL\\Nichol und Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nikita\\Zotero\\storage\\QHNRSCAR\\2102.html:text/html},
}

@misc{dhariwal_diffusion_2021,
	title = {Diffusion Models Beat {GANs} on Image Synthesis},
	url = {http://arxiv.org/abs/2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an {FID} of 2.97 on {ImageNet} 128\${\textbackslash}times\$128, 4.59 on {ImageNet} 256\${\textbackslash}times\$256, and 7.72 on {ImageNet} 512\${\textbackslash}times\$512, and we match {BigGAN}-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving {FID} to 3.94 on {ImageNet} 256\${\textbackslash}times\$256 and 3.85 on {ImageNet} 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	number = {{arXiv}:2105.05233},
	publisher = {{arXiv}},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	urldate = {2022-07-13},
	date = {2021-06-01},
	eprinttype = {arxiv},
	eprint = {2105.05233 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}
@inproceedings{li2014efficient,
  title={Efficient mini-batch training for stochastic optimization},
  author={Li, Mu and Zhang, Tong and Chen, Yuqiang and Smola, Alexander J},
  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={661--670},
  year={2014}
}

@misc{https://doi.org/10.48550/arxiv.1505.04597,
  doi = {10.48550/ARXIV.1505.04597},
  
  url = {https://arxiv.org/abs/1505.04597},
  
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{https://doi.org/10.48550/arxiv.2208.09392,
  doi = {10.48550/ARXIV.2208.09392},
  
  url = {https://arxiv.org/abs/2208.09392},
  
  author = {Bansal, Arpit and Borgnia, Eitan and Chu, Hong-Min and Li, Jie S. and Kazemi, Hamid and Huang, Furong and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{wang2023comprehensive,
      title={A Comprehensive Survey of Continual Learning: Theory, Method and Application}, 
      author={Liyuan Wang and Xingxing Zhang and Hang Su and Jun Zhu},
      year={2023},
      eprint={2302.00487},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shin2017continual,
      title={Continual Learning with Deep Generative Replay}, 
      author={Hanul Shin and Jung Kwon Lee and Jaehong Kim and Jiwon Kim},
      year={2017},
      eprint={1705.08690},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{maracani2021recall,
      title={RECALL: Replay-based Continual Learning in Semantic Segmentation}, 
      author={Andrea Maracani and Umberto Michieli and Marco Toldo and Pietro Zanuttigh},
      year={2021},
      eprint={2108.03673},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ronneberger_unet,
      title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, 
      author={Olaf Ronneberger and Philipp Fischer and Thomas Brox},
      year={2015},
      eprint={1505.04597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{Fukushima1980NeocognitronAS,
  title={Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  author={Kunihiko Fukushima},
  journal={Biological Cybernetics},
  year={1980},
  volume={36},
  pages={193-202}
}
@misc{Goodfellow_2017,
  doi = {10.48550/ARXIV.1701.00160},
  url = {https://arxiv.org/abs/1701.00160},
  author = {Goodfellow, Ian},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {NIPS 2016 Tutorial: Generative Adversarial Networks},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{guo2016entity,
      title={Entity Embeddings of Categorical Variables}, 
      author={Cheng Guo and Felix Berkhahn},
      year={2016},
      eprint={1604.06737},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{gretton2008kernel,
      title={A Kernel Method for the Two-Sample Problem}, 
      author={Arthur Gretton and Karsten Borgwardt and Malte J. Rasch and Bernhard Scholkopf and Alexander J. Smola},
      year={2008},
      eprint={0805.2368},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{esteban2017realvalued,
      title={Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs}, 
      author={Cristóbal Esteban and Stephanie L. Hyland and Gunnar Rätsch},
      year={2017},
      eprint={1706.02633},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{maaten2008tsne,
      title={Visualizing Data using t-SNE}, 
      author={Laurens van der Maaten and Geoffrey Hinton},
      year={2008},
}
@misc{LSMsource,
  title = {London Smart Meter dataset},
  howpublished = {\url{https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households}},
  note = {Accessed: 2022}
}
@misc{OMsource,
  title = {Openmeter dataset},
  howpublished = {\url{https://appstore.logarithmo.de/app/openmeterplatform/v1/demo/page-datenuebersicht?lang=DE}},
  note = {Accessed: 2023}
}
@misc{ACNsource,
  title = {ACN dataset},
  howpublished = {\url{https://ev.caltech.edu/dataset}},
  note = {Accessed: 2023}
}
@misc{rasul2021autoregressive,
      title={Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting}, 
      author={Kashif Rasul and Calvin Seward and Ingmar Schuster and Roland Vollgraf},
      year={2021},
      eprint={2101.12072},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}